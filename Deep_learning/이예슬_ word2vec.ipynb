{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Word2Vec\nhere I implement word2vec with very simple example using tensorflow  \nword2vec is vector representation for words with similarity"}, {"metadata": {}, "cell_type": "markdown", "source": "# Collect Data\nwe will use only 10 sentences to create word vectors"}, {"metadata": {}, "cell_type": "code", "source": "corpus = ['king is a strong man', \n          'queen is a wise woman', \n          'boy is a young man',\n          'girl is a young woman',\n          'prince is a young king',\n          'princess is a young queen',\n          'man is strong', \n          'woman is pretty',\n          'prince is a boy will be king',\n          'princess is a girl will be queen']", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Remove stop words\nIn order for efficiency of creating word vector, we will remove commonly used words"}, {"metadata": {}, "cell_type": "code", "source": "def remove_stop_words(corpus):\n    stop_words = ['is', 'a', 'will', 'be']\n    results = []\n    for text in corpus:\n        tmp = text.split(' ')\n        for stop_word in stop_words:\n            if stop_word in tmp:\n                tmp.remove(stop_word)\n        results.append(\" \".join(tmp))\n    \n    return results", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "corpus = remove_stop_words(corpus)", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "words = []\nfor text in corpus:\n    for word in text.split(' '):\n        words.append(word)\n\nwords = set(words)", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "here we have word set by which we will have word vector"}, {"metadata": {}, "cell_type": "code", "source": "words", "execution_count": 5, "outputs": [{"output_type": "execute_result", "execution_count": 5, "data": {"text/plain": "{'boy',\n 'girl',\n 'king',\n 'man',\n 'pretty',\n 'prince',\n 'princess',\n 'queen',\n 'strong',\n 'wise',\n 'woman',\n 'young'}"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# data generation\nwe will generate label for each word using skip gram.  "}, {"metadata": {}, "cell_type": "code", "source": "x=enumerate(words)\nfor i, key in x:\n    print(i, \":\", key, \":\", x)\nprint()", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "0 : princess : <enumerate object at 0x7f7578c33d38>\n1 : king : <enumerate object at 0x7f7578c33d38>\n2 : wise : <enumerate object at 0x7f7578c33d38>\n3 : pretty : <enumerate object at 0x7f7578c33d38>\n4 : man : <enumerate object at 0x7f7578c33d38>\n5 : prince : <enumerate object at 0x7f7578c33d38>\n6 : strong : <enumerate object at 0x7f7578c33d38>\n7 : girl : <enumerate object at 0x7f7578c33d38>\n8 : boy : <enumerate object at 0x7f7578c33d38>\n9 : young : <enumerate object at 0x7f7578c33d38>\n10 : woman : <enumerate object at 0x7f7578c33d38>\n11 : queen : <enumerate object at 0x7f7578c33d38>\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "word2int = {}\n\nfor i,word in enumerate(words):\n    word2int[word] = i", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sentences = []\nfor sentence in corpus:\n    sentences.append(sentence.split())\n    \nWINDOW_SIZE = 2\n\ndata = []\nfor sentence in sentences:\n    for idx, word in enumerate(sentence):\n        for neighbor in sentence[max(idx - WINDOW_SIZE, 0) : min(idx + WINDOW_SIZE, len(sentence)) + 1] : \n            if neighbor != word:\n                data.append([word, neighbor])", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nfor text in corpus:\n    print(text)", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "king strong man\nqueen wise woman\nboy young man\ngirl young woman\nprince young king\nprincess young queen\nman strong\nwoman pretty\nprince boy king\nprincess girl queen\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "df = pd.DataFrame(data, columns = ['input', 'label'])\ndf.head(20)", "execution_count": 10, "outputs": [{"output_type": "execute_result", "execution_count": 10, "data": {"text/plain": "     input   label\n0     king  strong\n1     king     man\n2   strong    king\n3   strong     man\n4      man    king\n5      man  strong\n6    queen    wise\n7    queen   woman\n8     wise   queen\n9     wise   woman\n10   woman   queen\n11   woman    wise\n12     boy   young\n13     boy     man\n14   young     boy\n15   young     man\n16     man     boy\n17     man   young\n18    girl   young\n19    girl   woman", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>king</td>\n      <td>strong</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>king</td>\n      <td>man</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>strong</td>\n      <td>king</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>strong</td>\n      <td>man</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>man</td>\n      <td>king</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>man</td>\n      <td>strong</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>queen</td>\n      <td>wise</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>queen</td>\n      <td>woman</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>wise</td>\n      <td>queen</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>wise</td>\n      <td>woman</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>woman</td>\n      <td>queen</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>woman</td>\n      <td>wise</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>boy</td>\n      <td>young</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>boy</td>\n      <td>man</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>young</td>\n      <td>boy</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>young</td>\n      <td>man</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>man</td>\n      <td>boy</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>man</td>\n      <td>young</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>girl</td>\n      <td>young</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>girl</td>\n      <td>woman</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "df.shape", "execution_count": 11, "outputs": [{"output_type": "execute_result", "execution_count": 11, "data": {"text/plain": "(52, 2)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "word2int", "execution_count": 12, "outputs": [{"output_type": "execute_result", "execution_count": 12, "data": {"text/plain": "{'princess': 0,\n 'king': 1,\n 'wise': 2,\n 'pretty': 3,\n 'man': 4,\n 'prince': 5,\n 'strong': 6,\n 'girl': 7,\n 'boy': 8,\n 'young': 9,\n 'woman': 10,\n 'queen': 11}"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Define Tensorflow Graph"}, {"metadata": {}, "cell_type": "code", "source": "import tensorflow as tf\nimport numpy as np\n\nONE_HOT_DIM = len(words)\n\n# function to convert numbers to one hot vectors\ndef to_one_hot_encoding(data_point_index):\n    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n    one_hot_encoding[data_point_index] = 1\n    return one_hot_encoding\n\nX = [] # input word\nY = [] # target word\n\nfor x, y in zip(df['input'], df['label']):\n    X.append(to_one_hot_encoding(word2int[ x ]))\n    Y.append(to_one_hot_encoding(word2int[ y ]))\n\n# convert them to numpy arrays\nX_train = np.asarray(X)\nY_train = np.asarray(Y)\n\n# making placeholders for X_train and Y_train\nx = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\ny_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n\n# word embedding will be 2 dimension for 2d visualization\nEMBEDDING_DIM = 2 \n\n# hidden layer: which represents word vector eventually\nW1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))\nb1 = tf.Variable(tf.random_normal([1])) #bias\nhidden_layer = tf.add(tf.matmul(x,W1), b1)\n\n# output layer\nW2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))\nb2 = tf.Variable(tf.random_normal([1]))\nprediction = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))\n\n# loss function: cross entropy\nloss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis=[1]))\n\n# training operation\ntrain_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "X_train", "execution_count": 14, "outputs": [{"output_type": "execute_result", "execution_count": 14, "data": {"text/plain": "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Train"}, {"metadata": {}, "cell_type": "code", "source": "sess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init) \n\niteration = 20000\nfor i in range(iteration):\n    # input is X_train which is one hot encoded word\n    # label is Y_train which is one hot encoded neighbor word\n    sess.run(train_op, feed_dict={x: X_train, y_label: Y_train})\n    if i % 3000 == 0:\n        print('iteration '+str(i)+' loss is : ', sess.run(loss, feed_dict={x: X_train, y_label: Y_train}))", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "iteration 0 loss is :  3.213951\niteration 3000 loss is :  1.8065629\niteration 6000 loss is :  1.7611676\niteration 9000 loss is :  1.741262\niteration 12000 loss is :  1.7291843\niteration 15000 loss is :  1.7206156\niteration 18000 loss is :  1.7139156\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# Now the hidden layer (W1 + b1) is actually the word look up table\nvectors = sess.run(W1 + b1)\nprint(vectors)", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "[[ 5.2287903  -1.8577751 ]\n [-0.6620965  -0.80360353]\n [ 4.447618   -0.22929727]\n [ 0.37840608  2.2666233 ]\n [-0.58101326 -0.5888581 ]\n [-1.3846998  -3.8833673 ]\n [-4.0382633  -5.6604886 ]\n [ 1.29232    -0.0940683 ]\n [-0.94574726 -1.7728543 ]\n [ 0.02937146  0.05395396]\n [ 1.539744   -0.7039583 ]\n [ 0.7794354  -0.03291473]]\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "# word vector in table"}, {"metadata": {}, "cell_type": "code", "source": "w2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2'])\nw2v_df['word'] = words\nw2v_df = w2v_df[['word', 'x1', 'x2']]\nw2v_df", "execution_count": 17, "outputs": [{"output_type": "execute_result", "execution_count": 17, "data": {"text/plain": "        word        x1        x2\n0   princess  5.228790 -1.857775\n1       king -0.662097 -0.803604\n2       wise  4.447618 -0.229297\n3     pretty  0.378406  2.266623\n4        man -0.581013 -0.588858\n5     prince -1.384700 -3.883367\n6     strong -4.038263 -5.660489\n7       girl  1.292320 -0.094068\n8        boy -0.945747 -1.772854\n9      young  0.029371  0.053954\n10     woman  1.539744 -0.703958\n11     queen  0.779435 -0.032915", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>x1</th>\n      <th>x2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>princess</td>\n      <td>5.228790</td>\n      <td>-1.857775</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>king</td>\n      <td>-0.662097</td>\n      <td>-0.803604</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>wise</td>\n      <td>4.447618</td>\n      <td>-0.229297</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>pretty</td>\n      <td>0.378406</td>\n      <td>2.266623</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>man</td>\n      <td>-0.581013</td>\n      <td>-0.588858</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>prince</td>\n      <td>-1.384700</td>\n      <td>-3.883367</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>strong</td>\n      <td>-4.038263</td>\n      <td>-5.660489</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>girl</td>\n      <td>1.292320</td>\n      <td>-0.094068</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>boy</td>\n      <td>-0.945747</td>\n      <td>-1.772854</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>young</td>\n      <td>0.029371</td>\n      <td>0.053954</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>woman</td>\n      <td>1.539744</td>\n      <td>-0.703958</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>queen</td>\n      <td>0.779435</td>\n      <td>-0.032915</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# word vector in 2d chart"}, {"metadata": {}, "cell_type": "code", "source": "import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nfor word, x1, x2 in zip(w2v_df['word'], w2v_df['x1'], w2v_df['x2']):\n    ax.annotate(word, (x1,x2 ))\n    \nPADDING = 1.0\nx_axis_min = np.amin(vectors, axis=0)[0] - PADDING\ny_axis_min = np.amin(vectors, axis=0)[1] - PADDING\nx_axis_max = np.amax(vectors, axis=0)[0] + PADDING\ny_axis_max = np.amax(vectors, axis=0)[1] + PADDING\n \nplt.xlim(x_axis_min,x_axis_max)\nplt.ylim(y_axis_min,y_axis_max)\nplt.rcParams[\"figure.figsize\"] = (5,5)\n\nplt.show()", "execution_count": 18, "outputs": [{"output_type": "display_data", "data": {"text/plain": "<Figure size 640x480 with 1 Axes>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}